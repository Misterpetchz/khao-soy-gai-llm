{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5SM9eXKcmLB"
      },
      "outputs": [],
      "source": [
        "pip install googletrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGfl3TIvcmsE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data augmentation"
      ],
      "metadata": {
        "id": "spcTMCPit6Vx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMVvXol9cmwk"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from googletrans import Translator\n",
        "from nltk.corpus import wordnet\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize translator\n",
        "translator = Translator()\n",
        "\n",
        "def synonym_replacement(text):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    random_word_list = list(set([word for word in words if wordnet.synsets(word)]))\n",
        "    random.shuffle(random_word_list)\n",
        "    num_replacements = random.randint(1, 3)\n",
        "    for random_word in random_word_list:\n",
        "        synonyms = wordnet.synsets(random_word)\n",
        "        if synonyms:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words = [synonym if word == random_word else word for word in new_words]\n",
        "        num_replacements -= 1\n",
        "        if num_replacements == 0:\n",
        "            break\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def back_translation(text):\n",
        "    try:\n",
        "        translated = translator.translate(text, src='th', dest='en').text\n",
        "        back_translated = translator.translate(translated, src='en', dest='th').text\n",
        "        return back_translated\n",
        "    except Exception as e:\n",
        "        print(f\"Error during back translation: {e}\")\n",
        "        return text\n",
        "\n",
        "def random_insertion(text):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    num_insertions = random.randint(1, 3)\n",
        "    for _ in range(num_insertions):\n",
        "        new_words.insert(random.randint(0, len(new_words)), random.choice(words))\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def random_swap(text):\n",
        "    words = text.split()\n",
        "    new_words = words.copy()\n",
        "    num_swaps = random.randint(1, 3)\n",
        "    for _ in range(num_swaps):\n",
        "        idx1, idx2 = random.sample(range(len(words)), 2)\n",
        "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def random_deletion(text):\n",
        "    words = text.split()\n",
        "    if len(words) == 1:\n",
        "        return text\n",
        "    new_words = [word for word in words if random.uniform(0, 1) > 0.25]\n",
        "    if not new_words:\n",
        "        return words[random.randint(0, len(words)-1)]\n",
        "    return ' '.join(new_words)\n",
        "\n",
        "def augment_text(text):\n",
        "    augmented_texts = [\n",
        "        synonym_replacement(text),\n",
        "        back_translation(text),\n",
        "        random_insertion(text),\n",
        "        random_swap(text),\n",
        "        random_deletion(text)\n",
        "    ]\n",
        "    return augmented_texts\n",
        "\n",
        "# Define the restaurant data\n",
        "restaurant_data = {\n",
        "    \"restaurant_name\": [\n",
        "        \"ร้านข้าวซอยนิมมาน\",\n",
        "        \"ร้านแกงฮังเลบ้านอาจารย์\",\n",
        "        \"ร้านน้ำพริกหนุ่มยอดนิยม\",\n",
        "        \"ร้านอาหารทะเลพัทยา\",\n",
        "        \"ร้านหมูกระทะชาวบ้าน\",\n",
        "        \"ร้านอาหารอีสานป้าแดง\",\n",
        "        \"ร้านคาเฟ่ในสวน\",\n",
        "        \"ร้านอาหารเวียดนามสุขใจ\"\n",
        "    ],\n",
        "    \"description\": [\n",
        "        \"เขียนคำอธิบายเมนูข้าวซอยไก่ของร้านผมให้หน่อย: ข้าวซอยไก่ของร้านเราถือเป็นเมนูซิกเนเจอร์ที่ห้ามพลาด! น้ำแกงข้าวซอยหอมกรุ่นจากเครื่องเทศหลากหลายชนิด เนื้อไก่ที่นุ่มละลายในปาก เส้นข้าวซอยเหนียวนุ่มทุกคำ เสิร์ฟพร้อมเครื่องเคียงผักสด เพิ่มความอร่อยด้วยบีบมะนาวเล็กน้อย รับรองว่าทุกคำที่ทานจะได้สัมผัสรสชาติที่เข้มข้นและกลมกล่อม\",\n",
        "        \"เขียนคำอธิบายเมนูแกงฮังเลของร้านผมให้หน่อย: แกงฮังเลบ้านอาจารย์คือเมนูที่นักท่องเที่ยวต้องมาลิ้มลอง! หมูสามชั้นนุ่มๆ ที่ถูกหมักจนหอมด้วยเครื่องแกงฮังเลสูตรพิเศษจากภาคเหนือ รสชาติเปรี้ยวหวานและเผ็ดน้อยๆ รับประทานคู่กับข้าวสวยร้อนๆ เป็นประสบการณ์การรับประทานอาหารที่ไม่เหมือนใคร\",\n",
        "        \"เขียนแคปชั่นเมนูเด็ดของร้านน้ำพริกหนุ่มยอดนิยมให้หน่อยขอแบบน่าดึงดูด: น้ำพริกหนุ่มยอดนิยม เผ็ดกำลังดี หอมเครื่องเทศและพริกคั่ว เสิร์ฟพร้อมผักสดและข้าวเหนียว อร่อยจนต้องลอง!\",\n",
        "        \"เขียนคำอธิบายเมนูปูผัดผงกะหรี่ของร้านผมให้หน่อย: ปูผัดผงกะหรี่ของร้านอาหารทะเลพัทยาเป็นเมนูที่คนรักอาหารทะเลไม่ควรพลาด! เนื้อปูสดๆ ผัดกับผงกะหรี่และเครื่องปรุงที่เข้มข้น รสชาติหวานมันกลมกล่อม ทานคู่กับข้าวสวยร้อนๆ ยิ่งเพิ่มความอร่อย\",\n",
        "        \"เขียนแคปชั่นเมนูเด็ดของร้านหมูกระทะชาวบ้านให้หน่อยขอแบบน่าดึงดูด: หมูกระทะชาวบ้าน เนื้อหมูสดใหม่ น้ำจิ้มรสเด็ด การันตีความอร่อยจนต้องมาลองเอง!\",\n",
        "        \"เขียนคำอธิบายเมนูส้มตำไทยของร้านผมให้หน่อย: ส้มตำไทยป้าแดง รสชาติจัดจ้านถึงใจ พริกสดตำมือ ผสมกับมะละกอกรอบๆ และเครื่องปรุงที่สดใหม่ เสิร์ฟพร้อมกับข้าวเหนียวและไก่ย่าง เป็นเมนูที่นักชิมไม่ควรพลาด\",\n",
        "        \"เขียนแคปชั่นเมนูเค้กช็อกโกแลตของร้านคาเฟ่ในสวนให้หน่อยขอแบบน่าดึงดูด: เค้กช็อกโกแลตโฮมเมด เนื้อนุ่มลิ้น หอมหวานทุกคำที่ทาน\",\n",
        "        \"เขียนคำอธิบายเมนูเฝอเนื้อของร้านผมให้หน่อย: เฝอเนื้อสุขใจ น้ำซุปรสชาติกลมกล่อม เนื้อวัวนุ่มๆ เส้นเฝอเหนียวนุ่ม พร้อมด้วยผักสดและเครื่องปรุงต่างๆ เป็นเมนูที่ให้ความรู้สึกอบอุ่นเหมือนทานอาหารที่บ้าน\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Define the food data\n",
        "food_data = {\n",
        "    \"task\": [\n",
        "        \"แนะนำอาหารอร่อย\",\n",
        "        \"แนะนำอาหารจานเด็ด\",\n",
        "        \"บอกพิกัดร้านอาหาร\",\n",
        "        \"ช่วยหาร้านอาหารน่ากินที่เชียงใหม่\",\n",
        "        \"แนะนำเมนูเด็ด\",\n",
        "        \"ช่วยหาร้านอาหารท้องถิ่น\",\n",
        "        \"แนะนำร้านอาหารวิวสวย\",\n",
        "        \"ช่วยหาร้านอาหารสำหรับครอบครัว\"\n",
        "    ],\n",
        "    \"description\": [\n",
        "        \"ข้าวซอยไก่ของร้านเราถือเป็นเมนูซิกเนเจอร์ที่ห้ามพลาด! น้ำแกงข้าวซอยหอมกรุ่นจากเครื่องเทศหลากหลายชนิด เนื้อไก่ที่นุ่มละลายในปาก เส้นข้าวซอยเหนียวนุ่มทุกคำ เสิร์ฟพร้อมเครื่องเคียงผักสด เพิ่มความอร่อยด้วยบีบมะนาวเล็กน้อย รับรองว่าทุกคำที่ทานจะได้สัมผัสรสชาติที่เข้มข้นและกลมกล่อม\",\n",
        "        \"แกงฮังเลบ้านอาจารย์คือเมนูที่นักท่องเที่ยวต้องมาลิ้มลอง! หมูสามชั้นนุ่มๆ ที่ถูกหมักจนหอมด้วยเครื่องแกงฮังเลสูตรพิเศษจากภาคเหนือ รสชาติเปรี้ยวหวานและเผ็ดน้อยๆ รับประทานคู่กับข้าวสวยร้อนๆ เป็นประสบการณ์การรับประทานอาหารที่ไม่เหมือนใคร\",\n",
        "        \"น้ำพริกหนุ่มยอดนิยม เผ็ดกำลังดี หอมเครื่องเทศและพริกคั่ว เสิร์ฟพร้อมผักสดและข้าวเหนียว อร่อยจนต้องลอง! ร้านตั้งอยู่ที่ตลาดสดแม่เหียะ ใกล้ๆ กับวัดแม่เหียะ เชียงใหม่\",\n",
        "        \"อยากกินอาหารน่ากินที่เชียงใหม่? แนะนำปูผัดผงกะหรี่ของร้านอาหารทะเลพัทยาในซอยนิมมาน เป็นเมนูที่คนรักอาหารทะเลไม่ควรพลาด! เนื้อปูสดๆ ผัดกับผงกะหรี่และเครื่องปรุงที่เข้มข้น รสชาติหวานมันกลมกล่อม ทานคู่กับข้าวสวยร้อนๆ ยิ่งเพิ่มความอร่อย\",\n",
        "        \"แนะนำเมนูเด็ดของร้านหมูกระทะชาวบ้าน: หมูกระทะชาวบ้าน เนื้อหมูสดใหม่ น้ำจิ้มรสเด็ด การันตีความอร่อยจนต้องมาลองเอง! ร้านตั้งอยู่ที่ถนนมูลเมือง ใกล้กับประตูท่าแพ เชียงใหม่\",\n",
        "        \"ช่วยหาร้านอาหารท้องถิ่น: ส้มตำไทยป้าแดง รสชาติจัดจ้านถึงใจ พริกสดตำมือ ผสมกับมะละกอกรอบๆ และเครื่องปรุงที่สดใหม่ เสิร์ฟพร้อมกับข้าวเหนียวและไก่ย่าง เป็นเมนูที่นักชิมไม่ควรพลาด ร้านตั้งอยู่ที่ถนนคนเดินเชียงใหม่\",\n",
        "        \"แนะนำร้านอาหารวิวสวย: เค้กช็อกโกแลตโฮมเมด เนื้อนุ่มลิ้น หอมหวานทุกคำที่ทาน ร้านคาเฟ่ในสวน บรรยากาศสวยงาม เหมาะสำหรับการนั่งพักผ่อนและถ่ายรูป\",\n",
        "        \"ช่วยหาร้านอาหารสำหรับครอบครัว: เฝอเนื้อสุขใจ น้ำซุปรสชาติกลมกล่อม เนื้อวัวนุ่มๆ เส้นเฝอเหนียวนุ่ม พร้อมด้วยผักสดและเครื่องปรุงต่างๆ เป็นเมนูที่ให้ความรู้สึกอบอุ่นเหมือนทานอาหารที่บ้าน ร้านนี้เหมาะสำหรับการมากับครอบครัว ตั้งอยู่ที่ถนนสุเทพ ใกล้มหาวิทยาลัยเชียงใหม่\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Define the branding data\n",
        "branding_data = {\n",
        "    \"task\": [\n",
        "        \"แนะนำกลยุทธ์การตลาด\",\n",
        "        \"ช่วยสร้างแบรนด์ร้านอาหาร\",\n",
        "        \"แนะนำวิธีโปรโมทร้านอาหาร\",\n",
        "        \"ขอคำแนะนำในการทำโฆษณา\",\n",
        "        \"ช่วยวางแผนการตลาดให้หน่อย\",\n",
        "        \"แนะนำวิธีเพิ่มยอดขาย\",\n",
        "        \"ช่วยหาแนวทางการตลาดที่เหมาะสม\",\n",
        "        \"ขอคำแนะนำในการใช้สื่อออนไลน์โปรโมทร้าน\"\n",
        "    ],\n",
        "    \"description\": [\n",
        "        \"การสร้างแบรนด์ร้านอาหารให้เป็นที่รู้จักต้องเริ่มจากการมีเอกลักษณ์ที่โดดเด่น การออกแบบโลโก้และบรรจุภัณฑ์ที่น่าสนใจ รวมถึงการใช้โซเชียลมีเดียในการโปรโมท\",\n",
        "        \"กลยุทธ์การตลาดที่ดีสำหรับร้านอาหารคือการใช้โปรโมชั่นดึงดูดลูกค้าใหม่ๆ และรักษาลูกค้าเก่า เช่น การแจกคูปองส่วนลด การจัดกิจกรรมพิเศษ\",\n",
        "        \"การโปรโมทร้านอาหารผ่านสื่อออนไลน์สามารถเพิ่มยอดขายได้ การใช้ภาพถ่ายที่น่าสนใจและการเขียนคำบรรยายที่ดึงดูดความสนใจเป็นสิ่งสำคัญ\",\n",
        "        \"การทำโฆษณาบนโซเชียลมีเดียเป็นวิธีที่มีประสิทธิภาพในการเข้าถึงกลุ่มเป้าหมาย การใช้วิดีโอสั้นๆ ที่แสดงให้เห็นถึงบรรยากาศและอาหารของร้านสามารถดึงดูดความสนใจได้\",\n",
        "        \"การวางแผนการตลาดสำหรับร้านอาหารควรเริ่มจากการวิเคราะห์กลุ่มเป้าหมาย และการตั้งเป้าหมายที่ชัดเจน การใช้สื่อออนไลน์และการทำโปรโมชั่นเป็นสิ่งสำคัญ\",\n",
        "        \"การเพิ่มยอดขายให้ร้านอาหารสามารถทำได้โดยการใช้เทคนิคการขายเพิ่ม เช่น การแนะนำเมนูพิเศษ การให้บริการที่ดี และการสร้างความประทับใจให้กับลูกค้า\",\n",
        "        \"การตลาดที่เหมาะสมสำหรับร้านอาหารควรคำนึงถึงความต้องการของลูกค้าและการสร้างประสบการณ์ที่ดี การใช้โปรโมชั่นและการโปรโมทผ่านสื่อออนไลน์เป็นสิ่งสำคัญ\",\n",
        "        \"การใช้สื่อออนไลน์ในการโปรโมทร้านอาหารสามารถเพิ่มยอดขายได้ การใช้ภาพถ่ายและวิดีโอที่น่าสนใจ การเขียนคำบรรยายที่ดึงดูดความสนใจ และการมีปฏิสัมพันธ์กับลูกค้าเป็นสิ่งสำคัญ\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Function to augment and save dataset\n",
        "def augment_and_save(data, filename, name_column, description_column):\n",
        "    df = pd.DataFrame(data)\n",
        "    augmented_data = {name_column: [], description_column: []}\n",
        "    for i, row in df.iterrows():\n",
        "        augmented_texts = augment_text(row[description_column])\n",
        "        for text in augmented_texts:\n",
        "            augmented_data[name_column].append(row[name_column])\n",
        "            augmented_data[description_column].append(text)\n",
        "    augmented_df = pd.DataFrame(augmented_data)\n",
        "    combined_df = pd.concat([df, augmented_df], ignore_index=True)\n",
        "    combined_df.to_csv(filename, index=False)\n",
        "    print(f\"Number of rows in the original dataset ({filename}):\", len(df))\n",
        "    print(f\"Number of rows in the augmented dataset ({filename}):\", len(augmented_df))\n",
        "    print(f\"Number of rows in the combined dataset ({filename}):\", len(combined_df))\n",
        "\n",
        "# Augment and save each dataset\n",
        "augment_and_save(restaurant_data, \"augmented_restaurant_data.csv\", \"restaurant_name\", \"description\")\n",
        "augment_and_save(food_data, \"augmented_food_data.csv\", \"task\", \"description\")\n",
        "augment_and_save(branding_data, \"augmented_branding_data.csv\", \"task\", \"description\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#fine-tune m-bert model"
      ],
      "metadata": {
        "id": "YeFQEzVgt2kp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "l_OdeQDJcttl",
        "collapsed": true,
        "outputId": "2cbcf801-741b-4075-ec41-b988fe4c6513"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'datasets'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-007acedcc5e9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load the augmented data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the augmented data\n",
        "restaurant_df = pd.read_csv(\"augmented_restaurant_data.csv\")\n",
        "food_df = pd.read_csv(\"augmented_food_data.csv\")\n",
        "branding_df = pd.read_csv(\"augmented_branding_data.csv\")\n",
        "\n",
        "# Combine all data into a single DataFrame\n",
        "combined_df = pd.concat([restaurant_df, food_df, branding_df], ignore_index=True)\n",
        "\n",
        "# Check for NaN values and drop them\n",
        "combined_df.dropna(subset=['description', 'task'], inplace=True)\n",
        "\n",
        "# Create labels for each task\n",
        "def label_task(task):\n",
        "    if 'ร้านอาหาร' in task:\n",
        "        return 0\n",
        "    elif 'อาหาร' in task:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "combined_df['label'] = combined_df['task'].apply(label_task)\n",
        "\n",
        "# Ensure the label column is integer type\n",
        "combined_df['label'] = combined_df['label'].astype(int)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_data(df):\n",
        "    return tokenizer(\n",
        "        df['description'].tolist(),\n",
        "        max_length=128,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "train_encodings = tokenize_data(train_df)\n",
        "test_encodings = tokenize_data(test_df)\n",
        "\n",
        "train_labels = torch.tensor(train_df['label'].values)\n",
        "test_labels = torch.tensor(test_df['label'].values)\n",
        "\n",
        "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
        "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)\n",
        "\n",
        "# Load the model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=3)\n",
        "model.cuda()\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "total_steps = len(train_dataloader) * 3\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Define training loop\n",
        "def train_model():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        b_input_ids = b_input_ids.cuda()\n",
        "        b_input_mask = b_input_mask.cuda()\n",
        "        b_labels = b_labels.cuda()\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Training loss: {avg_train_loss}\")\n",
        "\n",
        "# Define evaluation loop\n",
        "def evaluate_model():\n",
        "    model.eval()\n",
        "    preds, true_labels = [], []\n",
        "    for batch in test_dataloader:\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        b_input_ids = b_input_ids.cuda()\n",
        "        b_input_mask = b_input_mask.cuda()\n",
        "        b_labels = b_labels.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
        "\n",
        "        logits = outputs.logits\n",
        "        preds.append(logits.detach().cpu().numpy())\n",
        "        true_labels.append(b_labels.detach().cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "    true_labels = np.concatenate(true_labels, axis=0)\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    accuracy = accuracy_score(true_labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, preds, average='weighted')\n",
        "    print(f\"Accuracy: {accuracy}\")\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "    print(f\"F1: {f1}\")\n",
        "\n",
        "# Training and evaluation\n",
        "for epoch in range(20):\n",
        "    print(f\"Epoch {epoch + 1}\")\n",
        "    train_model()\n",
        "    evaluate_model()\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"./fine-tuned-mbert\")\n",
        "tokenizer.save_pretrained(\"./fine-tuned-mbert\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pythainlp"
      ],
      "metadata": {
        "id": "nl4YiH6Dti4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#use fine-tune m-bert to predict agent"
      ],
      "metadata": {
        "id": "fDOeXYsdtqVh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG5UnVo4dDTT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from huggingface_hub import login\n",
        "import re\n",
        "import string\n",
        "from pythainlp.tokenize import word_tokenize as thai_word_tokenize\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "from pythainlp.util import normalize\n",
        "from datetime import datetime\n",
        "\n",
        "# Set the Hugging Face token directly in the script (for testing purposes only)\n",
        "huggingface_token = \"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\"\n",
        "\n",
        "# Log in to Hugging Face using the token\n",
        "login(token=huggingface_token)\n",
        "\n",
        "# Set the API key directly in the script (for testing purposes only)\n",
        "os.environ[\"OPENTYPHOON_API_KEY\"] = \"sk-pbtHFB2O8idAeyrz2PqRqZk5c8LpX7CRYCzQqcswUZ9cofPk\"\n",
        "\n",
        "# Ensure the environment variable is set correctly\n",
        "api_key = os.environ.get(\"OPENTYPHOON_API_KEY\")\n",
        "if not api_key:\n",
        "    raise ValueError(\"API key not found. Please set the OPENTYPHOON_API_KEY environment variable.\")\n",
        "\n",
        "class PreprocessAgent:\n",
        "    def __init__(self):\n",
        "        self.stop_words = set(thai_stopwords())\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        # Normalize text\n",
        "        data = normalize(data)\n",
        "        # Remove special characters and punctuation\n",
        "        data = re.sub(f\"[{re.escape(string.punctuation)}]\", '', data)\n",
        "        # Tokenize\n",
        "        tokens = thai_word_tokenize(data)\n",
        "        # Remove stopwords\n",
        "        tokens = [token for token in tokens if token not in self.stop_words]\n",
        "        # Join tokens back to string\n",
        "        preprocessed_data = ' '.join(tokens)\n",
        "        return preprocessed_data\n",
        "\n",
        "class ChatHistoryAgent:\n",
        "    def __init__(self, log_file=\"chat_history.log\"):\n",
        "        self.log_file = log_file\n",
        "\n",
        "    def log_interaction(self, user_input, system_response):\n",
        "        with open(self.log_file, \"a\", encoding=\"utf-8\") as f:\n",
        "            timestamp = datetime.now().isoformat()\n",
        "            f.write(f\"{timestamp} - User: {user_input}\\n\")\n",
        "            f.write(f\"{timestamp} - System: {system_response}\\n\\n\")\n",
        "\n",
        "class ManagerAgent:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "        self.preprocess_agent = PreprocessAgent()\n",
        "        self.restaurant_agent = RestaurantAgent(api_key)\n",
        "        self.food_agent = FoodAgent(api_key)\n",
        "        self.branding_agent = BrandingAgent(api_key)\n",
        "        self.postprocess_agent = PostprocessAgent()\n",
        "        self.chat_history_agent = ChatHistoryAgent()\n",
        "\n",
        "        # Load the fine-tuned model and tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"./fine-tuned-mbert\")\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\"./fine-tuned-mbert\")\n",
        "\n",
        "    def predict_agent(self, data):\n",
        "        # Tokenize the input data\n",
        "        inputs = self.tokenizer(data, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "        # Perform prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Get the predicted class\n",
        "        predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
        "\n",
        "        # Map the prediction to the appropriate agent\n",
        "        if predicted_class == 0:\n",
        "            return 'restaurant'\n",
        "        elif predicted_class == 1:\n",
        "            return 'food'\n",
        "        elif predicted_class == 2:\n",
        "            return 'branding'\n",
        "        else:\n",
        "            return 'unknown'\n",
        "\n",
        "    def handle_request(self, request):\n",
        "        try:\n",
        "            data = request.get(\"data\")\n",
        "\n",
        "            if not data:\n",
        "                return {\"error\": \"Missing data\"}\n",
        "\n",
        "            # Preprocess the data\n",
        "            preprocessed_data = self.preprocess_agent.preprocess(data)\n",
        "\n",
        "            # Predict the appropriate agent based on the preprocessed data\n",
        "            task_type = self.predict_agent(preprocessed_data)\n",
        "\n",
        "            if task_type == 'unknown':\n",
        "                return {\"error\": \"Unable to determine the appropriate agent\"}\n",
        "\n",
        "            # Handle the request with the appropriate agent\n",
        "            if task_type == 'restaurant':\n",
        "                response = self.restaurant_agent.handle_task(preprocessed_data)\n",
        "            elif task_type == 'food':\n",
        "                response = self.food_agent.handle_task(preprocessed_data)\n",
        "            elif task_type == 'branding':\n",
        "                response = self.branding_agent.handle_task(preprocessed_data)\n",
        "            else:\n",
        "                return {\"error\": \"Invalid task type\"}\n",
        "\n",
        "            # Postprocess the response\n",
        "            final_response = self.postprocess_agent.postprocess(response)\n",
        "\n",
        "            # Log the interaction\n",
        "            self.chat_history_agent.log_interaction(data, final_response)\n",
        "\n",
        "            return final_response\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "class RestaurantAgent:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def handle_task(self, data):\n",
        "        system_prompt = (\n",
        "            \"คุณเป็นผู้เชี่ยวชาญด้านการแนะนำร้านอาหารและการเขียนคำอธิบายสำหรับเนื้อหาของร้านอาหาร \"\n",
        "            \"คุณต้องพูดคุยกับผู้ใช้เหมือนเพื่อน ค่อยๆ สอบถามความชอบของผู้ใช้และแนะนำร้านอาหารที่เหมาะสมที่สุด \"\n",
        "            \"พร้อมทั้งเขียนคำอธิบายที่น่าสนใจสำหรับเนื้อหาของร้านอาหารเพื่อดึงดูดลูกค้า \"\n",
        "            \"คำอธิบายควรรวมถึงรายละเอียดเกี่ยวกับอาหารแต่ละจาน เช่น ส่วนผสม รสชาติ และบรรยากาศของร้าน \"\n",
        "            \"ตอบกลับให้เป็นกันเองและช่วยเหลือในการเลือกอาหารที่เหมาะสมที่สุดสำหรับผู้ใช้ \"\n",
        "            \"เช่น แนะนำร้านอาหารสำหรับครอบครัว บรรยากาศดี ราคาไม่แพง เป็นต้น\"\n",
        "        )\n",
        "        combined_data = system_prompt + \"\\n\\n\" + \"คำขอ: \" + data\n",
        "        return get_typhoon_response(combined_data, self.api_key, task=\"restaurant\")\n",
        "\n",
        "class FoodAgent:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def handle_task(self, data):\n",
        "        system_prompt = (\n",
        "            \"คุณเป็นผู้เชี่ยวชาญด้านการแนะนำอาหาร \"\n",
        "            \"คุณต้องพูดคุยกับผู้ใช้เหมือนเพื่อน ค่อยๆ สอบถามความชอบของผู้ใช้และแนะนำอาหารที่เหมาะสมที่สุด \"\n",
        "            \"พร้อมทั้งเขียนคำอธิบายที่น่าสนใจสำหรับอาหารเพื่อดึงดูดลูกค้า \"\n",
        "            \"คำอธิบายควรรวมถึงรายละเอียดเกี่ยวกับอาหารแต่ละจาน เช่น ส่วนผสม รสชาติ และประสบการณ์การรับประทาน \"\n",
        "            \"ตอบกลับให้เป็นกันเองและช่วยเหลือในการเลือกอาหารที่เหมาะสมที่สุดสำหรับผู้ใช้ \"\n",
        "            \"เช่น แนะนำอาหารท้องถิ่น อาหารที่เป็นเอกลักษณ์ และอาหารตามฤดูกาล เป็นต้น\"\n",
        "        )\n",
        "        combined_data = system_prompt + \"\\n\\n\" + \"คำขอ: \" + data\n",
        "        return get_typhoon_response(combined_data, self.api_key, task=\"food\")\n",
        "\n",
        "class BrandingAgent:\n",
        "    def __init__(self, api_key):\n",
        "        self.api_key = api_key\n",
        "\n",
        "    def handle_task(self, data):\n",
        "        system_prompt = (\n",
        "            \"คุณเป็นผู้เชี่ยวชาญด้านการสร้างแบรนด์สำหรับร้านอาหาร \"\n",
        "            \"คุณต้องพูดคุยกับผู้ใช้เหมือนเพื่อน ค่อยๆ สอบถามความต้องการและเป้าหมายของผู้ใช้ในการสร้างแบรนด์ \"\n",
        "            \"พร้อมทั้งเขียนคำแนะนำที่น่าสนใจสำหรับการสร้างแบรนด์ที่โดดเด่น \"\n",
        "            \"คำแนะนำควรรวมถึงรายละเอียดเกี่ยวกับกลยุทธ์การตลาด การสร้างความเป็นเอกลักษณ์ของแบรนด์ และวิธีการดึงดูดลูกค้า \"\n",
        "            \"ตอบกลับให้เป็นกันเองและช่วยเหลือในการสร้างแบรนด์ที่เหมาะสมที่สุดสำหรับผู้ใช้ \"\n",
        "            \"เช่น แนะนำการใช้สื่อสังคมออนไลน์ การจัดกิจกรรม และการออกแบบบรรจุภัณฑ์ เป็นต้น\"\n",
        "        )\n",
        "        combined_data = system_prompt + \"\\n\\n\" + \"คำขอ: \" + data\n",
        "        return get_typhoon_response(combined_data, self.api_key, task=\"branding\")\n",
        "\n",
        "class PostprocessAgent:\n",
        "    def postprocess(self, data):\n",
        "        # Implement any postprocessing logic here (e.g., data formatting, summarization)\n",
        "        return data.strip()\n",
        "\n",
        "def get_llm_prediction(prompt, api_key):\n",
        "    base_url = \"https://api.opentyphoon.ai/v1\"\n",
        "    url = f\"{base_url}/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": \"typhoon-instruct\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 300,\n",
        "        \"temperature\": 0.6,\n",
        "        \"top_p\": 1\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    result = response.json()\n",
        "    predicted_task = result['choices'][0]['message']['content'].strip().lower()\n",
        "\n",
        "    # Map the prediction to the appropriate agent\n",
        "    if \"restaurant\" in predicted_task:\n",
        "        return 'restaurant'\n",
        "    elif \"food\" in predicted_task:\n",
        "        return 'food'\n",
        "    elif \"branding\" in predicted_task:\n",
        "        return 'branding'\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "def get_typhoon_response(prompt, api_key, task):\n",
        "    base_url = \"https://api.opentyphoon.ai/v1\"\n",
        "    url = f\"{base_url}/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    payload = {\n",
        "        \"model\": \"typhoon-instruct\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 500,\n",
        "        \"temperature\": 0.6,\n",
        "        \"top_p\": 1\n",
        "    }\n",
        "\n",
        "    response = requests.post(url, headers=headers, json=payload)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    result = response.json()\n",
        "    return result['choices'][0]['message']['content'].strip()\n",
        "\n",
        "def qa_loop():\n",
        "    manager = ManagerAgent(api_key)\n",
        "\n",
        "    print(\"Welcome to the QA system. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() == \"exit\":\n",
        "            print(\"Exiting the QA system. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        request = {\"data\": user_input}\n",
        "        response = manager.handle_request(request)\n",
        "\n",
        "        print(\"System: \")\n",
        "        print(response)\n",
        "\n",
        "# Start the QA loop\n",
        "qa_loop()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}